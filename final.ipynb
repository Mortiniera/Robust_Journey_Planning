{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content\n",
    "* [Library imports](#lib_imports)\n",
    "* [Data import & Preprocessing](#Data_import_preprocessing)\n",
    "* [Algorithm description & Helpers definition](#algorithm_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports <a class=\"anchor\" id=\"lib_imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"helpers.py\")\n",
    "from helpers import *\n",
    "\n",
    "from collections import defaultdict\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pyspark.sql.functions as func\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from numpy import array\n",
    "import timeit\n",
    "import itertools\n",
    "import operator\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import getpass\n",
    "from pyspark.streaming import StreamingContext\n",
    "from  pyspark.streaming.kafka import KafkaUtils, OffsetRange\n",
    "\n",
    "from branca.colormap import LinearColormap\n",
    "import folium\n",
    "import folium.plugins as plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TIME = pd.Timestamp(2150, 1, 1, 0, 0, 0)\n",
    "MIN_TIME = pd.Timestamp(2000, 1, 1, 0, 0, 0)\n",
    "MAX_WALK_DIST = 500\n",
    "METERS_PER_MIN = 85\n",
    "source_trip_id = 'origin_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Pre-processing <a class=\"anchor\" id=\"Data_import_preprocessing\"></a>\n",
    "In this section we will define generic functions for our imports and pre-process the data to remove unusable samples and format informations such as dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used not to perform main processing twice\n",
    "# computes data structures from main data\n",
    "reload_df = False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_df:\n",
    "    username = getpass.getuser()\n",
    "\n",
    "    # Use this when running on the cluster\n",
    "    os.environ['PYSPARK_PYTHON'] = '/opt/anaconda3/bin/python'\n",
    "    spark = (SparkSession\n",
    "             .builder\n",
    "             .appName('streaming-{0}'.format(username))\n",
    "             .master('yarn')\n",
    "             .config('spark.executor.memory', '1g')\n",
    "             .config('spark.executor.instances', '2')\n",
    "             .config('spark.executor.cores', '2')\n",
    "             .config('spark.jars.packages', 'org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.2')\n",
    "             .getOrCreate())\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "    conf = sc.getConf()\n",
    "\n",
    "    spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We considered that delays are random variables modelling lateness per stop and trip id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_df:\n",
    "    delays = spark.read.parquet(\"hdfs://iccluster042.iccluster.epfl.ch:8020/user/hive/homes/dpetresc/\"+\\\n",
    "                                \"hive/sbb_delay_mean_avg_tripid_stopname/delta_0000001_0000001_0000\")\n",
    "    delays_df = delays.toPandas()\n",
    "    delays_dict = defaultdict(lambda : defaultdict(lambda: {}))\n",
    "    for iStop in delays_df.index.levels[0][:]:\n",
    "        for jTrip in delays_df.loc[iStop].index[:]:\n",
    "            delays_dict[iStop][jTrip] = delays_df.loc[(iStop, jTrip)].values\n",
    "    delays = {k: dict(v) for k, v in delays_dict.items()}\n",
    "    save_pkl(delays, 'delays_dict')\n",
    "delays = load_pkl('delays_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renames columns in english names\n",
    "# parses timestamps for trip_date and scheduled times\n",
    "# removes samples for which the trip does not stop\n",
    "# removes samples for which there is not at least one arrival and departure time (it be scheduled or observed)\n",
    "def pre_process(raw):\n",
    "    raw = raw.toDF(*new_columns)\n",
    "    # parse timestamp\n",
    "    raw = raw.withColumn('trip_date', func.to_timestamp(raw.trip_date, 'dd.MM.yyyy').alias('trip_date'))\n",
    "    # remove trains that do not stop\n",
    "    raw = raw.filter(func.col('stop_at_station')=='false').drop('stop_at_station')\n",
    "    # Keep only trips that have at least one arrival and departure time\n",
    "    raw = raw.filter((func.col('sch_arr_time').isNotNull() | func.col('real_arr_time').isNotNull()) & \\\n",
    "                     (func.col('sch_dep_time').isNotNull() | func.col('real_dep_time').isNotNull())\n",
    "                    )\n",
    "    # parse scheduled arrival and departure times\n",
    "    raw = raw.withColumn('sch_arr_time', func.to_timestamp(raw.sch_arr_time, 'dd.MM.yyyyHH:mm').alias('sch_arr_time'))\n",
    "    raw = raw.withColumn('sch_dep_time', func.to_timestamp(raw.sch_dep_time, 'dd.MM.yyyy HH:mm').alias('sch_dep_time'))\n",
    "    \n",
    "    # compute list of trips that appear more than once THIS IS SLOW, should try to do it on main data\n",
    "    invalid_ids = raw.groupBy('trip_id').agg(func.count('*').alias('trip_id_counts')).filter(func.col('trip_id_counts')<=1)\n",
    "    # put it as a list\n",
    "    invalid_ids = invalid_ids.select('trip_id').distinct().rdd.map(lambda r: r[0]).collect()\n",
    "    # remove trips for which there is only a single sample\n",
    "    raw = raw.where(raw.trip_id.isin(invalid_ids) == False)\n",
    "    raw = raw.withColumn('bpuic', raw['bpuic'].cast(IntegerType()))\n",
    "    return raw\n",
    "\n",
    "df = 0\n",
    "if reload_df:\n",
    "    df = pre_process(import_today_tomorrow(pd.Timestamp('2018-02-18'), spark))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes one-to-one mapping from stopnames to bpuics and bpuics to index between 0 and (N-1) where N is number of stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bpuic</th>\n",
       "      <th>y_Coord_Est</th>\n",
       "      <th>x_Coord_Nord</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8508186</td>\n",
       "      <td>628463</td>\n",
       "      <td>220751</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8590028</td>\n",
       "      <td>599934</td>\n",
       "      <td>200621</td>\n",
       "      <td>1882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     bpuic  y_Coord_Est  x_Coord_Nord   idx\n",
       "0  8508186       628463        220751   400\n",
       "6  8590028       599934        200621  1882"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_bpuic_map = {}\n",
    "bpuic_stop_map = {}\n",
    "bpuic_index_map = {}\n",
    "index_bpuic_map = {}\n",
    "\n",
    "geo_data = pd.read_csv('geo_stops_data.csv', sep=';')\n",
    "\n",
    "if reload_df:\n",
    "    stop_bpuic = [t for t in df.select('bpuic', 'stop_name').distinct().collect() if t.bpuic in geo_data.bpuic.values]\n",
    "    stop_bpuic_map, bpuic_stop_map, bpuic_index_map, index_bpuic_map = get_maps(stop_bpuic)\n",
    "    save_pkl(stop_bpuic_map, 'sb_map')\n",
    "    save_pkl(bpuic_stop_map, 'bs_map')\n",
    "    save_pkl(bpuic_index_map, 'bi_map')\n",
    "    save_pkl(index_bpuic_map, 'ib_map')\n",
    "else: \n",
    "    stop_bpuic_map = load_pkl('sb_map')\n",
    "    bpuic_stop_map = load_pkl('bs_map')\n",
    "    bpuic_index_map = load_pkl('bi_map')\n",
    "    index_bpuic_map = load_pkl('ib_map')\n",
    "\n",
    "geo_data = geo_data[geo_data.bpuic.apply(lambda x: x in bpuic_index_map)]\n",
    "geo_data['idx'] = geo_data.bpuic.apply(lambda x: bpuic_index_map[x])\n",
    "geo_data.set_index('idx')\n",
    "geo_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute euclidian distance between two stops given by their index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(idx1, idx2):\n",
    "    r1 = geo_data_indexed.loc[idx1]\n",
    "    r2 = geo_data_indexed.loc[idx2]\n",
    "    x1 = r1.x_Coord_Nord\n",
    "    y1 = r1.y_Coord_Est\n",
    "    x2 = r2.x_Coord_Nord\n",
    "    y2 = r2.y_Coord_Est\n",
    "    return euclidean_dist(x1, y1, x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute dict of set of tuples (id, time) rpresenting for each stop the neighboors reachable by walk and the associated walking time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 ms, sys: 70 µs, total: 11.3 ms\n",
      "Wall time: 10.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_bpuics = len(bpuic_index_map)\n",
    "geo_data_indexed = geo_data.set_index('idx')\n",
    "\n",
    "if reload_df:\n",
    "    # find neighboors that have walking time less than 5 minutes\n",
    "    bpuic_dists = defaultdict(lambda: set())\n",
    "    for iB in range(num_bpuics)[:]:\n",
    "        for jB in range(num_bpuics)[iB+1:]:\n",
    "            d = dist(iB, jB)\n",
    "            if d < MAX_WALK_DIST:\n",
    "                bpuic_dists[iB].add((jB, np.ceil(d/METERS_PER_MIN)))\n",
    "                bpuic_dists[jB].add((iB, np.ceil(d/METERS_PER_MIN)))\n",
    "    save_pkl(dict(bpuic_dists), 'bpuic_dists')\n",
    "else:\n",
    "    bpuic_dists = load_pkl('bpuic_dists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the connections either by increasing departure time or by decreasing arrival time.\n",
    "A connection represent a direct link between two stops exiting in the timetable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.79 s, sys: 773 ms, total: 2.56 s\n",
      "Wall time: 2.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dep_filename = 'by_dep_connections'\n",
    "arr_filename = 'by_arr_connections'\n",
    "if reload_df:\n",
    "    load_connections_to_pickle(dep_filename, arr_filename, df)   \n",
    "\n",
    "dep_connections = load_pkl(\"./{}\".format(dep_filename))\n",
    "arr_connections = load_pkl(\"./{}\".format(arr_filename))\n",
    "dep_connections[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Algorithm description & Helpers definition <a class=\"anchor\" id=\"algorithm_description\"></a>\n",
    "In this section we will briefly discuss which function are needed in order to implement the journey planning algorithm as well as explain how we will implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm description\n",
    "In order to create our own implementation of CSA (connection scan algorithm), we already computed all the connections from the timetable.\n",
    "#### CSA \n",
    "CSA is a straightforward algorithm that computes the earliest arrival time at target t from source s given a list of connections in the form (stop_a, stop_b, departure_time, arrival_time) in linear time of the number of connections.\n",
    "##### Reachable Stops\n",
    "In CSA, we iterate over the connections in order of increasing departure time. We define a connection to be reachable if as we iterate over this connection, the earliest arrival time at stop_a is smaller than the departure_time of the connection or if we are already in the same trip earlier. The latter condition is a sub condition of the first in our implementation.\n",
    "Moreover, we defined a trip as a succession of connections.\n",
    "##### Arrival or Departure\n",
    "Our implementation supports both the initial problem of finding earliest arrival time at target t and the reverse one of finding latest departure time from source s. We further explain only the earliest arrival time problem as the reverse is similar.\n",
    "#### Intermediate results\n",
    "Our first implement of the vanilla problem was able to compute earliest arrival time in half a second. We then extend our implementation to compute the corresponding journeys and then to consider also the probablity of never missing a connection during the journey.\n",
    "#### Probability Tradeoff\n",
    "Our fisrt idea was to consider optimal Pareto set of journeys with parameters probability of no miss, earliest arrival time, latest departure time, nb of legs. However, we quickly noticed that the task was a lot more complex than exepected (both implementation and mostly running time). We decided to choose the following tradeoff: at each stop we compute the earliest arrival time linked to the accumulated probability of no miss. We sliced what we considered reasonable probabilities (> 80%) as 2% slices.\n",
    "While scanning a connection, we compute the probability of no miss for every slice for which the connection is reachable and then update the target earliest times according to the newly computed probabilities. \n",
    "We only update these probabilities if they have either better probability or smaller arrival time.\n",
    "For example, we could have reached stop_a at 12:00 with probability 90% or at 12:10 w.p 95% or at 12:20 w.p 97% if we now scan a connection from stop_a to stop_b at 12:11 then the probability of no miss is p1 for slice 90%, p2 for slice 95% and 1 for slice 97%.\n",
    "At stop_b, we now store at slice 90% * p1 with the connection departure_time and slice 95% * p2 with the same time. If the two results land on the same slice, we only keep the best probability of both.\n",
    "Our implementation without further preprocessing ran in roughly 6 minutes.\n",
    "#### Additional feature\n",
    "Each time we update a stop earliest arrival times, we also update its neighbors defined as 5 minute close stops by walk. We compute the walk time between each neighbors and update their arrival time accounting for the walk delay\n",
    "\n",
    "#### Filtering Preprocess\n",
    "In order to speed our process up, we performed a few operations over the connection list.\n",
    "1. We remove any connection that departs before our journey starts\n",
    "2. We run the vanilla algorithm once and remove any connection that is not reachable before we run the heavier version\n",
    "3. We add as a condition of reachability that a stop should be reachable in at most 5 legs\n",
    "4. We limit our journey to 10h duration, this allows us to remove any connection departing 10 hours or more after the user-required departure time\n",
    "\n",
    "#### Results\n",
    "The last optimization make the heavy algorithm with probability tradeoff run under 2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108.03488372093024, 131.40505880977503)\n",
      "CPU times: user 119 µs, sys: 25 µs, total: 144 µs\n",
      "Wall time: 147 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Retrieves the mean and standard deviation of the delay for a given trip at a given stop\n",
    "def get_mean_std(trip, stop):\n",
    "    vals = delays[stop][trip]\n",
    "    mean, std = vals[0], vals[1]\n",
    "    return mean, std\n",
    "print(get_mean_std('80:06____:17400:000', 'Schaffhausen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3356\n",
      "CPU times: user 953 µs, sys: 30 µs, total: 983 µs\n",
      "Wall time: 859 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# computes the probability of missing a connection from trip_c1 to trip_c2 at stop_name. \n",
    "# offset_change accounts for an additional minute when changing trip at the same station\n",
    "def p_miss(delta_t, trip_c1, trip_c2, stop_name, offset_change = 60, n_samples = 10000):\n",
    "    #delta_t, trip_c1, trip_c2, stop_name = attr\n",
    "\n",
    "    delta_t -= offset_change\n",
    "    if stop_name not in delays:\n",
    "        return 0.0\n",
    "    if (trip_c1 not in delays[stop_name]) or (trip_c2 not in delays[stop_name]):\n",
    "        return 0.0\n",
    "    mean_d1, std_d1 = get_mean_std(trip_c1, stop_name)\n",
    "    mean_d2, std_d2 = get_mean_std(trip_c2, stop_name)\n",
    "    s = np.random.normal(mean_d1 - mean_d2, np.sqrt(std_d1*std_d1 + std_d2*std_d2), n_samples)\n",
    "    return (s > delta_t).mean()\n",
    "print(p_miss(120, '80:06____:17400:000', '80:06____:17402:000', 'Schaffhausen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.  , 0.98, 0.96, 0.94, 0.92, 0.9 , 0.88, 0.86, 0.84, 0.82])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define probability slices, from min_p to 1.0 each bucket_size\n",
    "min_p = 0.8\n",
    "bucket_size = 0.02\n",
    "proba_list_size = int((10 - 10*min_p)/(bucket_size*10)) + 1\n",
    "proba_buckets = np.linspace(min_p, 1.0, proba_list_size)[:0:-1]\n",
    "proba_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "# helper that finds the index corresponding to the given probability in proba_buckets\n",
    "def find_p_index(p):\n",
    "    if p < min_p:\n",
    "        return -1\n",
    "    return proba_list_size - int(np.ceil((int(100*p)-int(100*min_p))/(100*bucket_size)))-1\n",
    "\n",
    "print(find_p_index(0.98))\n",
    "print(find_p_index(0.31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_neighbors(departure, op, id_, earl_times, tupl, bucket_idx, neighbors_dict):\n",
    "    p_target, time_arr, trip_id, idx_source, footpath, iTrip_id, time_dep = tupl\n",
    "    if id_ in neighbors_dict:\n",
    "        neighbors_tuples = neighbors_dict[id_]\n",
    "        for (n_id, n_t) in neighbors_tuples:\n",
    "            new_arr_time = time_arr + timedelta(minutes = n_t) if \\\n",
    "                departure else time_arr - timedelta(minutes = n_t)\n",
    "            neighbor_tupl = earl_times[n_id][bucket_idx]\n",
    "\n",
    "            if op(new_arr_time, neighbor_tupl[1]):\n",
    "                    if (bucket_idx != 0 and op(new_arr_time, earl_times[n_id][bucket_idx-1][1]))\\\n",
    "                                or bucket_idx==0:\n",
    "                            earl_times[n_id][bucket_idx] = (p_target, new_arr_time, trip_id, idx_source,\\\n",
    "                                                            True, iTrip_id, time_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes a boolean flag for each connection for the preprocessing steps\n",
    "def get_flags(departure, connections, start, source, neighbors_dict):\n",
    "    \n",
    "    limit_temp = MAX_TIME if departure else MIN_TIME\n",
    "    connections_min_time = min(connections, key = lambda t: t[2])\n",
    "    time = start.replace(year = connections_min_time[2].year)\\\n",
    "        .replace(month = connections_min_time[2].month)\\\n",
    "        .replace(day = connections_min_time[2].day)\n",
    "    MAX_MIN_STAMP = time + pd.Timedelta(hours=10) if departure else time - pd.Timedelta(hours=10)\n",
    "    earl_times = [limit_temp]*len(bpuic_index_map)\n",
    "    valid_connections = [True]*len(connections)\n",
    "    earl_times[source] = time\n",
    "    list_trip = [(0, 0)]*len(bpuic_index_map)\n",
    "    list_trip[source] = (0, source_trip_id)\n",
    "    \n",
    "    op = operator.le if departure else operator.ge\n",
    "    \n",
    "    for idx_c, iConnection in enumerate(connections):\n",
    "        \n",
    "        if int(iConnection[0]) in bpuic_index_map and int(iConnection[1]) in bpuic_index_map:\n",
    "            \n",
    "            idx_source = bpuic_index_map[int(iConnection[0])]\n",
    "            idx_target = bpuic_index_map[int(iConnection[1])]\n",
    "            idx_tmp = idx_source\n",
    "            idx_source = idx_source if departure else idx_target\n",
    "            idx_target = idx_target if departure else idx_tmp\n",
    "            legs, arr_trip = list_trip[idx_source]\n",
    "\n",
    "            time_dep = iConnection[2]\n",
    "            time_arr = iConnection[3]\n",
    "            time_tmp = time_dep\n",
    "            time_dep = time_dep if departure else time_arr\n",
    "            time_arr = time_arr if departure else time_tmp\n",
    "            #print(time, time_dep, op(time, time_dep))\n",
    "            trip_id = iConnection[4]\n",
    "            if op(time, time_dep) and op(earl_times[idx_source], time_dep) \\\n",
    "                and (op(time_dep, MAX_MIN_STAMP)) and (legs <=5):\n",
    "\n",
    "                if op(time_arr, earl_times[idx_target]):\n",
    "                    earl_times[idx_target] = time_arr\n",
    "\n",
    "                    if arr_trip != trip_id:\n",
    "                        list_trip[idx_target] = (legs+1, trip_id)\n",
    "                    else:\n",
    "                        list_trip[idx_target] = (legs, trip_id)\n",
    "\n",
    "                    if idx_target in neighbors_dict:\n",
    "                        neighbors_tuples = neighbors_dict[idx_target]\n",
    "                        for (n_id, n_t) in neighbors_tuples:\n",
    "                            new_arr_time = time_arr + timedelta(minutes = n_t) if departure \\\n",
    "                                else time_arr - timedelta(minutes = n_t)\n",
    "                            neighbor_time = earl_times[n_id]\n",
    "\n",
    "                            if op(new_arr_time, neighbor_time):\n",
    "                                earl_times[n_id] = new_arr_time\n",
    "                                list_trip[n_id] = (legs, trip_id)\n",
    "            else:\n",
    "                valid_connections[idx_c] = False\n",
    "        else:\n",
    "            valid_connections[idx_c] = False\n",
    "    return valid_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per connection update loop\n",
    "def update_time(time, earliest_arrival, iConnection, departure=True):\n",
    "    if int(iConnection[0]) in bpuic_index_map and int(iConnection[1]) in bpuic_index_map:\n",
    "        \n",
    "        idx_source = bpuic_index_map[int(iConnection[0])]\n",
    "        idx_target = bpuic_index_map[int(iConnection[1])]\n",
    "        idx_tmp = idx_source\n",
    "        idx_source = idx_source if departure else idx_target\n",
    "        idx_target = idx_target if departure else idx_tmp\n",
    "\n",
    "        op = operator.le if departure else operator.ge\n",
    "        time_dep = iConnection[2]\n",
    "        time_arr = iConnection[3]\n",
    "        time_tmp = time_dep\n",
    "        time_dep = time_dep if departure else time_arr\n",
    "        time_arr = time_arr if departure else time_tmp\n",
    "        trip_id = iConnection[4]\n",
    "        #print(time, time_dep)\n",
    "        if op(time, time_dep):\n",
    "            l = [tupl for tupl in earliest_arrival[idx_source] if op(tupl[1], time_dep)]\n",
    "            for iTuple in l:\n",
    "                iTrip_id = iTuple[2]\n",
    "                source_time = iTuple[1]\n",
    "                if iTrip_id != trip_id and iTrip_id != source_trip_id:\n",
    "                    #print(2)\n",
    "                    # compute miss proba\n",
    "                    delta_t = time_dep - source_time if departure else source_time - time_dep\n",
    "                    delta_t = delta_t.seconds\n",
    "                    p = 1-p_miss(delta_t, trip_id, iTrip_id, bpuic_stop_map[index_bpuic_map[idx_source]])\n",
    "                    p_target = p*iTuple[0]\n",
    "                else:\n",
    "                    #print(3)\n",
    "                    p_target = iTuple[0]\n",
    "                p_bucket_idx = find_p_index(p_target)\n",
    "                if p_bucket_idx == -1:\n",
    "                    continue;\n",
    "                target_tuple = earliest_arrival[idx_target][p_bucket_idx]\n",
    "                if op(time_arr, target_tuple[1]):\n",
    "                    if (p_bucket_idx != 0 and op(time_arr, earliest_arrival[idx_target][p_bucket_idx-1][1]))\\\n",
    "                        or p_bucket_idx==0:\n",
    "                        res_tupl = (p_target, time_arr, trip_id, idx_source, False, iTrip_id, time_dep)\n",
    "                        earliest_arrival[idx_target][p_bucket_idx] = res_tupl\n",
    "                        update_neighbors(departure, op, idx_target, earliest_arrival, res_tupl, p_bucket_idx, bpuic_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise structures then loops over connections\n",
    "\n",
    "def find_earliest_time(departure, connections, limit_temp, \\\n",
    "                         start, dest, start_stamp):\n",
    "    origin = start if departure else dest\n",
    "    flags = get_flags(departure, connections[:], start_stamp, origin, bpuic_dists)\n",
    "    filtered_connections = list(map(lambda x: x[0], filter(lambda x: x[1], zip(connections, flags))))\n",
    "    \n",
    "    connections_min_time = min(filtered_connections, key = lambda t: t[2]) #if departure \\\n",
    "        #else max(filtered_connections, key = lambda t: t[2])\n",
    "    time = start_stamp.replace(year = connections_min_time[2].year)\\\n",
    "        .replace(month = connections_min_time[2].month)\\\n",
    "        .replace(day = connections_min_time[2].day)\n",
    "    \n",
    "    earliest_arrival = [[(1.0, limit_temp, '-', 0, False, '-', limit_temp) for j in range(proba_list_size)] \\\n",
    "                            for i in range(len(bpuic_index_map))]\n",
    "      \n",
    "    \n",
    "    earliest_arrival[origin][0] = (1.0, time, source_trip_id, origin, False, source_trip_id, time)\n",
    "    #list_trip[origin] = (-1,-1,-1)\n",
    "    for iConnection in filtered_connections:\n",
    "        update_time(time, earliest_arrival, iConnection, departure)\n",
    "    return earliest_arrival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then process the journeys found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_infos_path(elem):\n",
    "    return elem[0], elem[1], elem[2], elem[3], elem[4], elem[5], elem[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_trip_idx(list_path, trip):\n",
    "    for idx, elem in enumerate(list_path):\n",
    "        proba, arr, trip_id, idx_source, by_foot, trip_id_origin, dep = get_infos_path(elem)\n",
    "        if trip_id == trip:\n",
    "            return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delet_loop_in_path(path):    \n",
    "    final_path = []\n",
    "    for i,p in enumerate(path):\n",
    "        seen_stops = {}\n",
    "        temp_path = []\n",
    "        for idx, connection in enumerate(p):\n",
    "            if connection[0] not in seen_stops:\n",
    "                seen_stops[connection[0]] = idx\n",
    "                temp_path.append(connection)\n",
    "            else: \n",
    "                temp_path = temp_path[:seen_stops[connection[0]]]\n",
    "                temp_path.append(connection)\n",
    "        final_path.append(temp_path)\n",
    "    return final_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_path(departure, earliest_arrival):\n",
    "    path = [[] for i in range(10)]\n",
    "    \n",
    "    if departure : \n",
    "        #We start from the target and go back to find the source (the correct path for all the proba)\n",
    "        for idx, elems in enumerate(earliest_arrival[target][:6]):\n",
    "            connections_seen = []\n",
    "            temp_localisation = target\n",
    "            temp_trip_idx = -1\n",
    "            proba_t, arr_t, trip_id_t, idx_source_t, by_foot_t, trip_id_origin_t, dep_t = get_infos_path(elems)\n",
    "            if trip_id_t != '-': \n",
    "                while temp_localisation != source:\n",
    "                    if temp_trip_idx == -1:\n",
    "                        arr = arr_t\n",
    "                        proba = proba_t\n",
    "                        trip_id = trip_id_t\n",
    "                        trip_id_origin = trip_id_origin_t\n",
    "                        idx_source = idx_source_t\n",
    "                        dep = dep_t\n",
    "                        by_foot = by_foot_t\n",
    "                    else:\n",
    "                        idx_new_target = find_trip_idx(earliest_arrival[temp_localisation], temp_trip_idx)\n",
    "                        proba, arr, trip_id, idx_source, by_foot, trip_id_origin, dep = get_infos_path(earliest_arrival[temp_localisation][idx_new_target])\n",
    "\n",
    "                    s = bpuic_stop_map[index_bpuic_map[idx_source]]\n",
    "                    t = bpuic_stop_map[index_bpuic_map[temp_localisation]]\n",
    "                    path[idx].append((s, t, dep, arr, trip_id, proba))  \n",
    "                    temp_localisation = idx_source\n",
    "                    temp_trip_idx = trip_id_origin\n",
    "\n",
    "        path = [i[::-1] for i in path[:]]\n",
    "        \n",
    "    else:\n",
    "        # Same but we need to inverse the source and the arrival point (add the correct new trip_id and time) \n",
    "        for idx, elems in enumerate(earliest_arrival[source][:6]):\n",
    "            connections_seen = []\n",
    "            temp_localisation = source\n",
    "            temp_trip_idx = -1\n",
    "            proba_t, arr_t, trip_id_t, idx_target_t, by_foot_t, trip_id_origin_t, dep_t = get_infos_path(elems)\n",
    "            if trip_id_t != '-': \n",
    "                while temp_localisation != target:\n",
    "                    if temp_trip_idx == -1:\n",
    "                        arr = arr_t\n",
    "                        proba = proba_t\n",
    "                        trip_id = trip_id_t\n",
    "                        trip_id_origin = trip_id_origin_t\n",
    "                        idx_target = idx_target_t\n",
    "                        dep = dep_t\n",
    "                        by_foot = by_foot_t\n",
    "                    else:\n",
    "                        idx_new_target = find_trip_idx(earliest_arrival[temp_localisation], temp_trip_idx)\n",
    "                        proba, arr, trip_id, idx_target, by_foot, trip_id_origin, dep = get_infos_path(earliest_arrival[temp_localisation][idx_new_target])\n",
    "\n",
    "                    s = bpuic_stop_map[index_bpuic_map[temp_localisation]]\n",
    "                    t = bpuic_stop_map[index_bpuic_map[idx_target]]\n",
    "                    path[idx].append((s, t, dep, arr, trip_id, proba)) \n",
    "                    temp_localisation = idx_target\n",
    "                    temp_trip_idx = trip_id_origin\n",
    "                    \n",
    "    return delet_loop_in_path(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can query with the wanted parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 45s, sys: 854 ms, total: 1min 46s\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# User parameters for the problem are\n",
    "# Source, Target, departure/arrival time, departure(true/false)\n",
    "\n",
    "dep_stop_name = 'Genève'\n",
    "arr_stop_name = 'Lausanne'\n",
    "\n",
    "# whether to perform earliest arrival time or latest departure time problem\n",
    "departure = True\n",
    "\n",
    "user_departure_time = pd.Timestamp(2019, 3, 15, 15, 10)\n",
    "user_arrival_time = pd.Timestamp(2019, 3, 15, 15, 43)\n",
    "\n",
    "\n",
    "source = bpuic_index_map[stop_bpuic_map[dep_stop_name]]\n",
    "target = bpuic_index_map[stop_bpuic_map[arr_stop_name]]\n",
    "\n",
    "if departure:\n",
    "    earliest_arrival = find_earliest_time(departure, dep_connections[:], MAX_TIME, \\\n",
    "                                                        source,  target, user_departure_time)\n",
    "else:\n",
    "    earliest_arrival = find_earliest_time(departure, arr_connections, MIN_TIME, \\\n",
    "                                                        source,  target, user_arrival_time)\n",
    "    \n",
    "final_path = find_path(departure, earliest_arrival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "journeys = list(filter(lambda x : x != [], final_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the different journeys using folium. A round represents a leg and the color represents the probability of the journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source : https://github.com/ValentinMinder/Swisstopo-WGS84-LV03/blob/master/scripts/py/wgs84_ch1903.py\n",
    "\n",
    "# Convert CH y/x to WGS lat\n",
    "def CHtoWGSlat(y, x):\n",
    "    # Axiliary values (% Bern)\n",
    "    y_aux = (y - 600000) / 1000000\n",
    "    x_aux = (x - 200000) / 1000000\n",
    "    lat = (16.9023892 + (3.238272 * x_aux)) + \\\n",
    "            - (0.270978 * pow(y_aux, 2)) + \\\n",
    "            - (0.002528 * pow(x_aux, 2)) + \\\n",
    "            - (0.0447 * pow(y_aux, 2) * x_aux) + \\\n",
    "            - (0.0140 * pow(x_aux, 3))\n",
    "    # Unit 10000\" to 1\" and convert seconds to degrees (dec)\n",
    "    lat = (lat * 100) / 36\n",
    "    return lat\n",
    "\n",
    "# Convert CH y/x to WGS long\n",
    "def CHtoWGSlng(y, x):\n",
    "    # Axiliary values (% Bern)\n",
    "    y_aux = (y - 600000) / 1000000\n",
    "    x_aux = (x - 200000) / 1000000\n",
    "    lng = (2.6779094 + (4.728982 * y_aux) + \\\n",
    "            + (0.791484 * y_aux * x_aux) + \\\n",
    "            + (0.1306 * y_aux * pow(x_aux, 2))) + \\\n",
    "            - (0.0436 * pow(y_aux, 3))\n",
    "    # Unit 10000\" to 1\" and convert seconds to degrees (dec)\n",
    "    lng = (lng * 100) / 36\n",
    "    return lng\n",
    "\n",
    "def swissTopo_to_WGS84(bpuic, y_east, x_north, idx) :\n",
    "    lat = CHtoWGSlat(y_east, x_north)\n",
    "    lng = CHtoWGSlng(y_east, x_north)\n",
    "    return  pd.Series([bpuic, lat, lng, int(idx)])\n",
    "    \n",
    "#bpuic, #latitude, #longitude, #idx\n",
    "wgs84_geo_data = geo_data.apply(lambda row : swissTopo_to_WGS84(row[0], row[1], row[2], row[3]), axis=1)\n",
    "wgs84_geo_data.columns = ['bpuic', 'latitude', 'longitude', 'idx']\n",
    "wgs84_geo_data.bpuic = wgs84_geo_data.bpuic.astype('int64')\n",
    "wgs84_geo_data.idx = wgs84_geo_data.idx.astype('int64')\n",
    "wgs84_geo_data['stop_name'] = wgs84_geo_data.bpuic.map(lambda x : bpuic_stop_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_journey_step(journey, n):\n",
    "    #Display journey informations\n",
    "    s1 = \"Journey {} : {} ---> {}. \\n\".format(n, journey[0][0], journey[-1][0])\n",
    "    s2 = \"Departure time : {} - Arrival time : {} \\n\". format(journey[0][1] , journey[-1][1])\n",
    "    print(s1 + \"\\t\" + s2)\n",
    "    if len(journey)>2:\n",
    "        print(\"Journey details :\")\n",
    "        for i in range(0, len(journey), 2):\n",
    "            A = journey[i]\n",
    "            B = journey[i+1]\n",
    "            print('\\t'+'{} : {} --> {} : {} with trip_id = {}'.format(A[0], A[1].time(), B[1].time(), B[0], A[2]))\n",
    "    print('--------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_path(stations, path, m, color_scale, i) :\n",
    "    all_stops = list(map(lambda x : x[0], path))\n",
    "    all_stops.append(path[-1][1])\n",
    "    \n",
    "    proba = path[-1][5] #proba of success for this trip\n",
    "    \n",
    "    n = 0 #Counter of stops we iterate over\n",
    "    prev = all_stops[0] #store previous stop\n",
    "    points = [] #list of points to store to draw polyline\n",
    "    \n",
    "    trips_ids = list(map(lambda x : x[4], path))\n",
    "    trips_taken = [] #store the different trips to detect change of connection\n",
    "    \n",
    "    journey_step = [] #changes to be displayed\n",
    "    \n",
    "    color_stop = color_scale(proba)\n",
    "    \n",
    "    \n",
    "    #Iterate over each stop and draw a new stop at each change of connection\n",
    "    for p in all_stops :\n",
    "        #Additionnal information to be displayed over the points\n",
    "        features_current = stations[stations['stop_name'] == p]\n",
    "        pop = \"{}, success_rate : {}%\"\\\n",
    "            .format(features_current.iloc[0,4], proba*100)\n",
    "        right = [features_current.iloc[0,1], features_current.iloc[0,2]]\n",
    "        \n",
    "        \n",
    "        if(n == 0) : #first stop\n",
    "            trips_taken.append(trips_ids[n])\n",
    "            folium.CircleMarker(location=right, color = color_stop, \\\n",
    "                                popup=pop, fill=True, radius=10).add_to(m)\n",
    "            journey_step.append([p, path[n][2], trips_ids[n]])\n",
    "        else :\n",
    "            if(n == (len(all_stops) - 1)) : #last stop\n",
    "                folium.CircleMarker(location=right, color = color_stop, \\\n",
    "                                    popup=pop, fill=True, radius=10).add_to(m)\n",
    "                journey_step.append([p, path[n-1][3], trips_ids[n-1]])\n",
    "            else :\n",
    "                if(n < (len(all_stops) -2) and trips_ids[n] not in trips_taken) :\n",
    "                    #Change of connection\n",
    "                    trips_taken.append(trips_ids[n])\n",
    "                    times = \", arrival : {}, departure : {}\".format(path[n-1][3].time(), path[n][2].time())\n",
    "                    folium.CircleMarker(location=right, color= color_stop, \\\n",
    "                                        popup=pop + times, fill=True, radius=5).add_to(m)\n",
    "                    \n",
    "                    #update changes to display later\n",
    "                    journey_step.append([p, path[n-1][3], trips_ids[n-1]])\n",
    "                    journey_step.append([p, path[n][2], trips_ids[n]])\n",
    "                prev = p\n",
    "            \n",
    "        features_prev = stations[stations['stop_name'] == prev]\n",
    "        left = [features_prev.iloc[0,1], features_prev.iloc[0,2]]\n",
    "        points.append(left)\n",
    "        points.append(right)\n",
    "        n += 1\n",
    "    folium.PolyLine(locations=points, color=color_scale(proba), weight=5).add_to(m)\n",
    "        \n",
    "    print_journey_step(journey_step, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_map(stations, path_list):    \n",
    "    #set the colorscale matching the probabilities of a successful journey\n",
    "    \n",
    "    color_scale = LinearColormap(['r', 'y', 'g'], vmin = 0.82, vmax = 1)\n",
    "    color_scale.caption = 'Colormap of journey success rate'\n",
    "    \n",
    "    #create the folium map that will contain all journeys\n",
    "    depart = stations[stations['stop_name'] == path_list[0][0][0]]\n",
    "    depart_lat = depart['latitude'].iloc[0]\n",
    "    depart_lon = depart['longitude'].iloc[0]\n",
    "    arr = stations[stations['stop_name'] == path_list[0][-1][1]]\n",
    "    arr_lat = arr['latitude'].iloc[0]\n",
    "    arr_lon = arr['longitude'].iloc[0]\n",
    "    m = folium.Map(location=[np.mean([depart_lat, arr_lat]), np.mean([depart_lon, arr_lon])], \\\n",
    "                       zoom_start=10,tiles='CARTODBPOSITRON')\n",
    "    \n",
    "    #some folium parameters (minimap, layercontrol, full screen, etc)\n",
    "   \n",
    "    mcg = plugins.MarkerCluster(control=False)\n",
    "    m.add_child(mcg)\n",
    "    m.add_child(plugins.MiniMap())\n",
    "    \n",
    "    m.add_child(color_scale)\n",
    "    plugins.Fullscreen(\n",
    "        position='topright',\n",
    "        title='Expand me',\n",
    "        title_cancel='Exit me',\n",
    "        force_separate_button=True\n",
    "    ).add_to(m)\n",
    "    \n",
    "    #iterate over the journey list, display each journey on the folium map\n",
    "    i = 1\n",
    "    for p in path_list :\n",
    "        g = plugins.FeatureGroupSubGroup(mcg, 'journey {}'.format(i))\n",
    "        m.add_child(g)\n",
    "        draw_path(stations, p, g, color_scale, i)\n",
    "        i += 1\n",
    "        \n",
    "    folium.LayerControl(Collapsed = False).add_to(m)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = draw_map(wgs84_geo_data, journeys)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save('journeys_Lausanne_Geneve.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
